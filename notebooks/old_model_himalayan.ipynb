{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previous workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read the latest news articles from the RSS source (fetch_data)\n",
    "2. Convert the news articles into a pandas dataframe, open the previous stored dataset, convert that into another pandas dataframe, and merge them, remove duplicates. \n",
    "3. Save the compiled file\n",
    "4. Read the compiled file\n",
    "5. Drop where url or content is null (for an extreme case)\n",
    "6. Perform level 1 analysis by adding the supporting keywords into a column as list\n",
    "7. Continue level 1 analysis by counting the rows with lists with len greater than 0\n",
    "8. Check level 2 and level 3 continuation by validating if the articles have certain phrases\n",
    "9. If condition met for level 2, for level 2 perform level 2 analysis by adding the supporting keywords into a column as list\n",
    "10. Continue level 2 analysis by counting the rows with lists with len greater than 0\n",
    "11. If condition met for level 3, for level 3 perform level 3 analysis by adding the supporting keywords into a column as list\n",
    "10. Continue level 3 analysis by counting the rows with lists with len greater than 0\n",
    "14. Store results (not here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Current Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read the latest news articles from the RSS source (fetch_data)\n",
    "2. Convert the news articles into a pandas dataframe, open the previous stored dataset, convert that into another pandas dataframe, and merge them, remove duplicates. \n",
    "6. Perform level 1 analysis if not already previouly done by adding the supporting keywords into a column as list\n",
    "7. Continue level 1 analysis by counting the rows with lists with len greater than 0\n",
    "8. Check level 2 and level 3 continuation by validating if the articles have certain phrases\n",
    "9. If condition met for level 2, for level 2 perform level 2 analysis by adding the supporting keywords into a column as list\n",
    "10. Continue level 2 analysis by counting the rows with lists with len greater than 0\n",
    "11. If condition met for level 3, for level 3 perform level 3 analysis by adding the supporting keywords into a column as list\n",
    "10. Continue level 3 analysis by counting the rows with lists with len greater than 0\n",
    "14. Store results (not here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is this fast?\n",
    "\n",
    "1. Instead of performing the analysis for all the newspapers, this only analyses new dataset\n",
    "2. articles are cleaned as they are parsed\n",
    "3. avoiding multiple read writes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import feedparser\n",
    "from datetime import date\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re  \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "import time\n",
    "\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_level1=['arabl land','avail data','bureau statist','busi survey','cens publi','cens pop','children employ','civil registr','collect method','commerci export','complet rate','consum electr','consum energ','data access','data collect','data compil','data entri','data manag','data releas','data standard','data user','demograph data','densit popul','develop data','difusion dat','direct statist','disaggreg data','electr access','electr consumpt','energi consumpt','establish survey','exchang rate','extern debt','fertil rate','food import','food product','gender gap','govern debt','govern statist','gross domest','gross nation','health expenditur','health survey','import marchandis','improv data','improv statist','indic measur','indic preci','inflat rate','institut statist','interest payment','intern tourism','irrig land','land use','life expect','livestock product','merchandis export','merchandis trade','model statist','mortal rate','multilater debt','nation account','nation statist','nation survey','national brut','national statist','open data','part revenus','pay gap','popul census','popul growth','popul rate','price index','produccion aliment','purchas power','qualiti data','receit fiscal','releas data','revenu fiscal','rural popul','servic export','statist agenc','statist author','statist avail','statist committe','statist data','statist depart','statist national','statist offic','statist servic','statist studi','survey catalogu','tax payment','tax revenu','trade balanc','unemploy rate','use data','water suppli','youth unemploy']\n",
    "\n",
    "data_level2=['accur','adequ','ambigu','ambÃ­gu','apropi','bancal','bias','confiabl','correct','deceit','deceiv','decept','defectu','delud','engan','equivoc','erreur','erro','erron','errone','error','exact','exat','fake','fallaci','faux','fiabl','generaliz','illus','imparcial','impartial','imprecis','improp','inaccur','incorrect','inexact','invalid','limit','manipul','mislead','mistaken','parcial','prec','precis','proper','reliabl','rigor','rigour','scientif','sol','solid','som','son','sound','spurious','tromp','trompeur','unbias','unreli','unscientif','unsound','vag','vagu','val','valid',]\n",
    "\n",
    "data_level3=['data manipul','lead question','manipul dat','report bias','sampl select','sampl size']\n",
    "\n",
    "data_level_indicator = [' cpi ', ' fdi ', ' gdp ', ' gnp ', ' hdi ', ' wdi ']\n",
    "\n",
    "filter_list=[' data ',' record ',' research ',' statistics ',' study ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lengths_of_keywords():\n",
    "    return len(data_level1), len(data_level2), len(data_level3)\n",
    "\n",
    "def cleanHTML(raw_html):\n",
    "    text = BeautifulSoup(raw_html, \"lxml\").text\n",
    "    word_tokens = word_tokenize(text.lower().rstrip()) \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    return ' '.join(filtered_sentence)\n",
    "\n",
    "def performRSSNew(url, newspaper):\n",
    "    all_links = []\n",
    "    NewsFeed = feedparser.parse(url)\n",
    "    entries = NewsFeed.entries\n",
    "    for entry in entries:\n",
    "            content =  cleanHTML(entry.content[0].value)\n",
    "            published = entry.published\n",
    "            temp_dict = ({'url': entry.link, \n",
    "            'content': content , \n",
    "            'newspaper': newspaper,\n",
    "            'published_date': published})\n",
    "            all_links.append(temp_dict)    \n",
    "    return all_links\n",
    "\n",
    "def level1_count(article):\n",
    "    if article:\n",
    "        keyword_list = []\n",
    "        for word in data_level1:\n",
    "            search_ = (r\"\\b\"+word.split()[0]+r\"[a-zA-Z]*\\s\\b\"+word.split()[1]+\"[a-zA-Z]*\")\n",
    "            if re.search(search_, article):\n",
    "                keyword_list.append(word)\n",
    "        for word in data_level_indicator:\n",
    "            if (word in article):\n",
    "                keyword_list.append(word)\n",
    "        return keyword_list\n",
    "    return []\n",
    "\n",
    "def level2_count(article):\n",
    "    if article:\n",
    "        keyword_list = []\n",
    "        for word in data_level2:\n",
    "            search_ = (r\"\\b\"+word+r\"[a-zA-Z]*\")\n",
    "            if re.search(search_, article):\n",
    "                keyword_list.append(word)\n",
    "        return keyword_list\n",
    "    return []\n",
    "\n",
    "\n",
    "def level3_count(article):\n",
    "    if article:\n",
    "        word_tokens = word_tokenize(article.lower().rstrip()) \n",
    "        filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "\n",
    "        article = ' '.join(filtered_sentence)  \n",
    "        keyword_list = []\n",
    "        for word in data_level3:\n",
    "            search_ = (r\"\\b\"+word+r\"[a-zA-Z]*\")\n",
    "            if re.search(search_, article):\n",
    "                keyword_list.append(word)\n",
    "        return keyword_list\n",
    "    return []\n",
    "\n",
    "\n",
    "\n",
    "def level_2_3_filter(article):\n",
    "    if article:\n",
    "        article = article.lower().rstrip()\n",
    "        for word in filter_list:\n",
    "            if word in article:\n",
    "                return 1\n",
    "        return 0\n",
    "    return 0\n",
    "\n",
    "def level_len(count_list):\n",
    "    return 1 if len(count_list)>0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_merge_analyze_data_new(reset_analysis = False):\n",
    "    start = time.time()\n",
    "    prev_HT = pd.read_csv('newspaper/static/datasets/all.csv')\n",
    "    \n",
    "#     prev_HT['content'] = prev_HT.content.apply(cleanHTML)\n",
    "#     prev_HT.drop('category', inplace=True, axis=1)\n",
    "    \n",
    "    himalayan_times_url = \"https://thehimalayantimes.com/feed/\"\n",
    "    raw_HT = pd.DataFrame(performRSSNew(himalayan_times_url)) \n",
    "    print ('fetch_data', time.time() - start)\n",
    "    df_HT = pd.concat([prev_HT, raw_HT], sort=False).drop_duplicates(subset='url', keep=\"first\").reset_index(drop=True) \n",
    "\n",
    "    print ('compile_data', time.time() - start)\n",
    "    \n",
    "    if (reset_analysis):\n",
    "        df_HT['level1'] = np.nan\n",
    "        df_HT['level2'] = np.nan\n",
    "        df_HT['level3'] = np.nan\n",
    "        print ('reset_data', time.time() - start)\n",
    " \n",
    "    df_HT['level1'] = df_HT.apply(lambda x: level1_count(x['content']) if pd.isnull(x.level1) else x.level1, axis=1)\n",
    "    df_HT['level_len'] = df_HT['level1'].apply(level_len)\n",
    "    \n",
    "    print ('level_1_analysis', time.time() - start)\n",
    "    \n",
    "    df_HT['level_2_3_valid'] = df_HT['content'].apply(level_2_3_filter)\n",
    "    print ('level_2_filter ', time.time() - start)\n",
    "    \n",
    "    df_HT['level2'] =  df_HT.apply(lambda x: level2_count(x['content']) if pd.isnull(x.level2) else x.level2, axis=1)\n",
    "    df_HT['level2_len'] = df_HT.level2.apply(level_len)\n",
    "    print ('level_2_analysis ', time.time() - start)\n",
    "    \n",
    "    df_HT['level3'] =  df_HT.apply(lambda x: level3_count(x['content']) if pd.isnull(x.level3) else x.level3, axis=1)\n",
    "    df_HT['level3_len'] = df_HT.level3.apply(level_len)\n",
    "    print ('level_3_analysis ', time.time() - start)\n",
    "    \n",
    "#     df_HT.to_csv('newspaper/static/datasets/ht.csv', index=False)\n",
    "    return df_HT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetch_data 0.9882938861846924\n",
      "compile_data 0.9962596893310547\n",
      "level_1_analysis 1.2743253707885742\n",
      "level_2_filter  1.397298812866211\n",
      "level_2_analysis  1.6033315658569336\n",
      "level_3_analysis  1.8226597309112549\n"
     ]
    }
   ],
   "source": [
    "df = fetch_merge_analyze_data_new(reset_analysis=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>published_date</th>\n",
       "      <th>level1</th>\n",
       "      <th>level_len</th>\n",
       "      <th>level_2_3_valid</th>\n",
       "      <th>level2</th>\n",
       "      <th>level3</th>\n",
       "      <th>level2_len</th>\n",
       "      <th>level3_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://thehimalayantimes.com/finance/analysis...</td>\n",
       "      <td>foreign aid plays critical role infrastructure...</td>\n",
       "      <td>Sun, 27 Sep 2015 14:12:37 +0000</td>\n",
       "      <td>[ gdp ]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[proper, reliabl]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://thehimalayantimes.com/finance/analysis...</td>\n",
       "      <td>experts opine loss incurred country calculated...</td>\n",
       "      <td>Sat, 26 Sep 2015 19:45:19 +0000</td>\n",
       "      <td>[inflat rate]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[prec, precis, sol]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://thehimalayantimes.com/finance/analysis...</td>\n",
       "      <td>kathmandu : coca-cola recently came campaign ,...</td>\n",
       "      <td>Sun, 20 Sep 2015 14:08:57 +0000</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[proper, som]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://thehimalayantimes.com/finance/analysis...</td>\n",
       "      <td>clarity government â plans programmes hydropow...</td>\n",
       "      <td>Sun, 20 Sep 2015 13:41:56 +0000</td>\n",
       "      <td>[ fdi ]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[proper]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://thehimalayantimes.com/finance/analysis...</td>\n",
       "      <td>working 66 days year , able prosper ? kathmand...</td>\n",
       "      <td>Sun, 13 Sep 2015 11:15:42 +0000</td>\n",
       "      <td>[ gdp ]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[limit, proper, sol]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4892</th>\n",
       "      <td>https://thehimalayantimes.com/nepal/majority-m...</td>\n",
       "      <td>kathmandu , november 9 five members nepal comm...</td>\n",
       "      <td>Tue, 10 Nov 2020 01:45:47 +0000</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4893</th>\n",
       "      <td>https://thehimalayantimes.com/nepal/three-apf-...</td>\n",
       "      <td>birgunj : government set three armed police fo...</td>\n",
       "      <td>Mon, 09 Nov 2020 16:20:58 +0000</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4894</th>\n",
       "      <td>https://thehimalayantimes.com/nepal/one-held-i...</td>\n",
       "      <td>hetauda : police sunday arrested one person co...</td>\n",
       "      <td>Mon, 09 Nov 2020 16:08:43 +0000</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4895</th>\n",
       "      <td>https://thehimalayantimes.com/nepal/dao-prohib...</td>\n",
       "      <td>lamjung : owing surge covid-19 transmission , ...</td>\n",
       "      <td>Mon, 09 Nov 2020 15:57:43 +0000</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4896</th>\n",
       "      <td>https://thehimalayantimes.com/kathmandu/action...</td>\n",
       "      <td>kathmandu : international non-government organ...</td>\n",
       "      <td>Mon, 09 Nov 2020 14:00:51 +0000</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4897 rows Ã 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    url  \\\n",
       "0     https://thehimalayantimes.com/finance/analysis...   \n",
       "1     https://thehimalayantimes.com/finance/analysis...   \n",
       "2     https://thehimalayantimes.com/finance/analysis...   \n",
       "3     https://thehimalayantimes.com/finance/analysis...   \n",
       "4     https://thehimalayantimes.com/finance/analysis...   \n",
       "...                                                 ...   \n",
       "4892  https://thehimalayantimes.com/nepal/majority-m...   \n",
       "4893  https://thehimalayantimes.com/nepal/three-apf-...   \n",
       "4894  https://thehimalayantimes.com/nepal/one-held-i...   \n",
       "4895  https://thehimalayantimes.com/nepal/dao-prohib...   \n",
       "4896  https://thehimalayantimes.com/kathmandu/action...   \n",
       "\n",
       "                                                content  \\\n",
       "0     foreign aid plays critical role infrastructure...   \n",
       "1     experts opine loss incurred country calculated...   \n",
       "2     kathmandu : coca-cola recently came campaign ,...   \n",
       "3     clarity government â plans programmes hydropow...   \n",
       "4     working 66 days year , able prosper ? kathmand...   \n",
       "...                                                 ...   \n",
       "4892  kathmandu , november 9 five members nepal comm...   \n",
       "4893  birgunj : government set three armed police fo...   \n",
       "4894  hetauda : police sunday arrested one person co...   \n",
       "4895  lamjung : owing surge covid-19 transmission , ...   \n",
       "4896  kathmandu : international non-government organ...   \n",
       "\n",
       "                       published_date         level1  level_len  \\\n",
       "0     Sun, 27 Sep 2015 14:12:37 +0000        [ gdp ]          1   \n",
       "1     Sat, 26 Sep 2015 19:45:19 +0000  [inflat rate]          1   \n",
       "2     Sun, 20 Sep 2015 14:08:57 +0000             []          0   \n",
       "3     Sun, 20 Sep 2015 13:41:56 +0000        [ fdi ]          1   \n",
       "4     Sun, 13 Sep 2015 11:15:42 +0000        [ gdp ]          1   \n",
       "...                               ...            ...        ...   \n",
       "4892  Tue, 10 Nov 2020 01:45:47 +0000             []          0   \n",
       "4893  Mon, 09 Nov 2020 16:20:58 +0000             []          0   \n",
       "4894  Mon, 09 Nov 2020 16:08:43 +0000             []          0   \n",
       "4895  Mon, 09 Nov 2020 15:57:43 +0000             []          0   \n",
       "4896  Mon, 09 Nov 2020 14:00:51 +0000             []          0   \n",
       "\n",
       "      level_2_3_valid                level2 level3  level2_len  level3_len  \n",
       "0                   0     [proper, reliabl]     []           1           0  \n",
       "1                   0   [prec, precis, sol]     []           1           0  \n",
       "2                   0         [proper, som]     []           1           0  \n",
       "3                   0              [proper]     []           1           0  \n",
       "4                   0  [limit, proper, sol]     []           1           0  \n",
       "...               ...                   ...    ...         ...         ...  \n",
       "4892                0                    []     []           0           0  \n",
       "4893                0                    []     []           0           0  \n",
       "4894                0                    []     []           0           0  \n",
       "4895                0                    []     []           0           0  \n",
       "4896                0                    []     []           0           0  \n",
       "\n",
       "[4897 rows x 10 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "newspapers = [('https://thehimalayantimes.com/feed/', 'ht'), \n",
    "              ('https://english.onlinekhabar.com/feed', 'ok'),\n",
    "              ('https://www.nepalitimes.com/feed/', 'nt'),\n",
    "              ('https://kathmandutribune.com/feed/', 'kt'),\n",
    "              ('http://english.lokaantar.com/feed/', 'lk'),\n",
    "              ('https://www.nepalisansar.com/feed/', 'ns'),\n",
    "              ('http://telegraphnepal.com/feed/', 'tn'),\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_merge_analyze_data_new(reset_analysis = False):\n",
    "    start = time.time()\n",
    "    prev_all = pd.read_csv('newspaper/static/datasets/all.csv')\n",
    "\n",
    "    for newspaper in newspapers:\n",
    "        temp = pd.DataFrame(performRSSNew(newspaper[0], newspaper[1]))\n",
    "        df_ALL = pd.concat([prev_all, temp], sort=False)\n",
    "    \n",
    "    df_ALL = df_ALL.drop_duplicates(subset='url', keep=\"first\").reset_index(drop=True) \n",
    "     \n",
    "    print ('fetch_data and compile_data', time.time() - start)\n",
    "    \n",
    "    if (reset_analysis):\n",
    "        df_ALL['level1'] = np.nan\n",
    "        df_ALL['level2'] = np.nan\n",
    "        df_ALL['level3'] = np.nan\n",
    "        print ('reset_data', time.time() - start)\n",
    " \n",
    "    df_ALL['level1'] = df_ALL.apply(lambda x: level1_count(x['content']) if pd.isnull(x.level1) else x.level1, axis=1)\n",
    "    df_ALL['level_len'] = df_ALL['level1'].apply(level_len)\n",
    "    \n",
    "    print ('level_1_analysis', time.time() - start)\n",
    "    \n",
    "    df_ALL['level_2_3_valid'] = df_ALL['content'].apply(level_2_3_filter)\n",
    "    print ('level_2_filter ', time.time() - start)\n",
    "    \n",
    "    df_ALL['level2'] =  df_ALL.apply(lambda x: level2_count(x['content']) if pd.isnull(x.level2) else x.level2, axis=1)\n",
    "    df_ALL['level2_len'] = df_ALL.level2.apply(level_len)\n",
    "    print ('level_2_analysis ', time.time() - start)\n",
    "    \n",
    "    df_ALL['level3'] =  df_ALL.apply(lambda x: level3_count(x['content']) if pd.isnull(x.level3) else x.level3, axis=1)\n",
    "    df_ALL['level3_len'] = df_ALL.level3.apply(level_len)\n",
    "    print ('level_3_analysis ', time.time() - start)\n",
    "    \n",
    "#     df_ALL.to_csv('newspaper/static/datasets/ht.csv', index=False)\n",
    "    return df_ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetch_data and compile_data 10.597536087036133\n",
      "level_1_analysis 10.762979745864868\n",
      "level_2_filter  10.908011198043823\n",
      "level_2_analysis  11.077815771102905\n",
      "level_3_analysis  11.251969575881958\n"
     ]
    }
   ],
   "source": [
    "df = fetch_merge_analyze_data_new(reset_analysis=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
